# ๐ง ุฏุฑุงุณุฉ ููุงุฑูุฉ ูุงุฎุชูุงุฑ ูููุฐุฌ LLM ููุชูุญ ุงููุตุฏุฑ ูุชุทุจูู ุฏุฑุฏุดุฉ ุนูู Google Colab (CPU)

### ุฅุนุฏุงุฏ: ุฃุญูุฏ ูุญูุฏ ุณุนูุฏ ุนูู  
**ุงูุณูุงู**: ุจุญุซ ุนู ูููุฐุฌ ุดุจูู ChatGPT ููู ููุชูุญ ุงููุตุฏุฑ ูููุฏุฑ ูุดุชุบู ุนูู Google Colab CPU

---

## ุฃูููุง: ุฃูุถู 3 ููุฏููุงุช Open Source ูู ููุน ChatGPT (LLMs)

### ูู ูุฆุงุช ูุฎุชููุฉ:

| ุงููุฆุฉ | ุงูููุฏูู | ูุจุฐุฉ |
|--------|-------------|--------|
| ุงูุฐูุงุก ุงูุนุงูู | Mistral 7B Instruct | ููู ูู ุงููุญุงุฏุซุงุช ูุงูููุทู |
| ุงูุฎูุฉ ูุงูููุงุกุฉ | Phi-2 | ุงูุฐูุงุก ูุงูุณุฑุนุฉ ูุน ุญุฌู ุตุบูุฑ |
| ุงูุญุฌู ุงูุฃูู | TinyLLaMA 1.1B | ูุง ูุญุชุงุฌ RAM ุนุงูู |

---

## ุงูุณุคุงู: ูู ููุตุฏ meta-llama/Llama-3.2-1B-Instructุ

ุงูุฅุฌุงุจุฉ: ูุนู. ูุฐุง ูู ุงููููุฐุฌ ุงูุฐู ูููู ุชุดุบููู ูู ุจูุฆุงุช CPU ูุซู Google Colab.

---

## ุฃูุง ุจุงุณุชุฎุฏู Google Colab (CPU Mode)

ูุนูู ุงูุฃุณุงุณ ุฏู ุงุญุชุงุฌูุง ูููุฐุฌ:
- ุตุบูุฑ ุญุฌููุง
- ูุชูุงูู ูุน transformers
- ููููุน ุนููู LoRA ูุญูู ุงูููุงุฐุฌ ูุญููุง


---

## ุงูููุงุฑูุฉ ุงูููุงุฆูุฉ:

| ุงููููุฐุฌ                  | ุงูุญุฌู   | ุงูุฐูุงุก ุงูุนุงู | ุงูููุงุฑุฏ | ุงูุณุฑุนุฉ | ุงููุบุฉ ุงูุนุฑุจูุฉ | ููุงุท ููุฉ | ููุงุท ุถุนู |
|--------------------------|---------|------------------|----------------------|------------------|------------------------|---------------|--------------|
| LLaMA-3.2-1B-Instruct    | 1B      | ุฌูุฏ ุฌุฏูุง         | ุฎููู                 | ูุชูุณุท             | ููุจูู                 | ุญุฏูุซ โ ููุฌู Instruct | ูููู ุงูุชูุณูู |
| Phi-2                    | 1.7B    | ููุชุงุฒ ูู ุงูููุทู  | ุฎููู ุฌุฏูุง            | ุณุฑูุน              | ุถุนูู                  | ุฐูุงุก ุนุงูู | ูุง ูููู ุนุฑุจู |
| TinyLLaMA-1.1B           | 1.1B    | ูุชูุณุท            | ุฃุฎููู                | ุณุฑูุน ุฌุฏูุง         | ุถุนูู ุฌุฏูุง             | ููุชุงุฒ ููุชุนูู | ุงุณุชุฌุงุจุงุช ุณุทุญูุฉ |

---

## ูุชูุฌุฉ ุงูุงุฎุชูุงุฑ:

**meta-llama/Llama-3.2-1B-Instruct** ูู ุงูููุฏูู ุงููุฎุชุงุฑ ููุฃุณุจุงุจ ุงูุชุงููุฉ:

1. ููุงุณุจ ูู CPU
2. ูุจูู ุนูู LLaMA 3.2 ุงูุญุฏูุซ
3. ูุฏุฑุจ ุนูู ููุท Instruct
4. ููุฏู ุงุณุชุฌุงุจุงุช ุฐููุฉ ูููุฌูุฉ

---

## ุงูุชุฌุฑุจุฉ ุงูุฃููู

```python
!pip install transformers accelerate

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# ุชุนุฑูู ุงููููุฐุฌ
model_id = "meta-llama/Llama-3.2-1B-Instruct"

# ุชุญููู ุงููููุฐุฌ
model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float32)
tokenizer = AutoTokenizer.from_pretrained(model_id)

prompt = "ุงุดุฑุญ ูู ุงูุฐูุงุก ุงูุงุตุทูุงุนู ุจุทุฑููุฉ ูุจุณุทุฉ."
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=150)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)
```

---

## ุงุณุชูุชุงุฌ ููุงุฆู

ูู ุฎูุงู ูุฐู ุงูุฏุฑุงุณุฉ ุชู ุงููุตูู ุฅูู ุฃู ูููุฐุฌ **LLaMA-3.2-1B-Instruct** ูู ุงูุฎูุงุฑ ุงูุฃูุซู ููู ูุฎุชุงุฑ ุงูุชุทููุฑ ุนูู Google Colab CPU ุจุงููุฌุงู. ููููู ุชุทููุฑู ูุญููุง ูุชุฏุนูู ุงููุบุฉ ุงูุนุฑุจูุฉ ูุชุดุบููู ูุน ูุดุงุฑูุน ูุซู eyEar ูุบูุฑูุง.

